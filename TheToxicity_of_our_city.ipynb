{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkHI0dr2I00z",
        "outputId": "b8ff90bf-e90c-4c80-b17e-dc298da17f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id    target                                       comment_text  \\\n",
            "0   59856  0.893617               haha you guys are a bunch of losers.   \n",
            "1  239607  0.912500  Yet call out all Muslims for the acts of a few...   \n",
            "2  239612  0.830769  This bitch is nuts. Who would read a book by a...   \n",
            "3  240311  0.968750                                   You're an idiot.   \n",
            "4  240329  0.900000  Who cares!? Stark trek and Star Wars fans are ...   \n",
            "\n",
            "   severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
            "0         0.021277  0.000000         0.021277  0.872340  0.0000    0.0   \n",
            "1         0.050000  0.237500         0.612500  0.887500  0.1125    0.0   \n",
            "2         0.107692  0.661538         0.338462  0.830769  0.0000    0.0   \n",
            "3         0.031250  0.062500         0.000000  0.968750  0.0000    NaN   \n",
            "4         0.100000  0.200000         0.000000  0.900000  0.0000    NaN   \n",
            "\n",
            "   atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
            "0      0.0  ...        2006  rejected      0    0    0      1         0   \n",
            "1      0.0  ...       26670  approved      0    0    0      1         0   \n",
            "2      0.0  ...       26674  rejected      0    0    0      0         0   \n",
            "3      NaN  ...       32846  rejected      0    0    0      0         0   \n",
            "4      NaN  ...       32846  rejected      0    0    0      0         0   \n",
            "\n",
            "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
            "0         0.000000                         4                        47  \n",
            "1         0.000000                         4                        80  \n",
            "2         0.061538                         4                        65  \n",
            "3         0.000000                         0                        32  \n",
            "4         0.300000                         0                        10  \n",
            "\n",
            "[5 rows x 45 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Фиксируем зерно для повторяемости результатов\n",
        "np.random.seed(0)\n",
        "\n",
        "# Загрузка датасета\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "print (data.head())\n",
        "\n",
        "# Извлекаем тексты комментариев\n",
        "comments = data[\"comment_text\"]\n",
        "\n",
        "# Целевая переменная: 1 — токсичный, если target > 0.7; иначе — 0\n",
        "target = (data[\"target\"] > 0.7).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1. Разделение на train и test\n",
        "--\n",
        "Почему делаем это: чтобы обучать модель на одной части данных, а проверять — на другой, независимой. Это важно для честной оценки качества."
      ],
      "metadata": {
        "id": "s6A0FEBmOJJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделим данные: 70% для обучения, 30% для теста\n",
        "X_train, X_test, y_train, y_test = train_test_split(comments, target, test_size=0.3, random_state=0)\n",
        "\n",
        "# Проверим размеры\n",
        "print(f\"Train: {X_train.shape[0]} комментариев\")\n",
        "print(f\"Test: {X_test.shape[0]} комментариев\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFWIMeHON-pF",
        "outputId": "46d25a01-5aa5-4d28-8c21-34f50fb6f6cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 63631 комментариев\n",
            "Test: 27271 комментариев\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2. Преобразование текста в числа через CountVectorizer\n",
        "--\n",
        "\n",
        "Почему CountVectorizer: он создает таблицу, где строки — комментарии, а столбцы — уникальные слова. Значения — сколько раз слово встречается в комментарии."
      ],
      "metadata": {
        "id": "icGuHUqvOaxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем векторизатор, который преобразует текст в числовые векторы по частоте слов\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Обучаем его на тренировочных текстах и применяем к train и test\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Посмотрим форму полученной матрицы признаков\n",
        "print(f\"Количество признаков: {X_train_vec.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ospES4jOho5",
        "outputId": "99704ff0-3d1e-412d-82ad-fc5908227af5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество признаков: 58152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3. Логистическая регрессия и accuracy\n",
        "--\n",
        "Почему логистическая регрессия: это простая и интерпретируемая модель, часто используется как baseline."
      ],
      "metadata": {
        "id": "Ej_wSqR4OsFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучаем логистическую регрессию с увеличенным количеством итераций\n",
        "classifier = LogisticRegression(max_iter=2000)\n",
        "classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Предсказания на тестовой выборке\n",
        "y_pred = classifier.predict(X_test_vec)\n",
        "\n",
        "# Accuracy — доля правильно классифицированных комментариев\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Точность (accuracy) на тесте: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9ZgYEbDO2d5",
        "outputId": "e7ef4467-0ec3-4c05-d12d-2370cc9694a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность (accuracy) на тесте: 0.9285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 4. Функция предсказания токсичности по тексту\n",
        "--\n",
        "позволяет в реальном времени проверять, как модель реагирует на новые тексты."
      ],
      "metadata": {
        "id": "8J3lgworPLFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_toxicity(comment):\n",
        "    vec = vectorizer.transform([comment])\n",
        "    prob = classifier.predict_proba(vec)[0][1]  # вероятность токсичности\n",
        "    return prob\n",
        "\n",
        "# Пример\n",
        "print(f\"Токсичность: {predict_toxicity('abomination'):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKJz_veXPZVR",
        "outputId": "94a5c230-5c2f-43b2-f483-275b454f0a03"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токсичность: 0.374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 5–6. Примеры предсказания на \"apples\" + вывод 10 самых токсичных слов\n",
        "--\n",
        "модель принимает решение на основе веса каждого слова. Мы можем увидеть, какие слова она считает \"токсичными\" по статистике."
      ],
      "metadata": {
        "id": "vKE0HDs6QYX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка конкретных комментариев\n",
        "print(\"Apples are stupid:\", predict_toxicity(\"Apples are stupid\"))\n",
        "print(\"I love apples:\", predict_toxicity(\"I love apples\"))\n",
        "\n",
        "# Получаем веса признаков (слов) от модели\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "weights = classifier.coef_[0]\n",
        "\n",
        "# Находим индексы самых \"токсичных\" слов — те, у которых вес наибольший\n",
        "top_toxic_ids = np.argsort(weights)[-10:][::-1]\n",
        "top_toxic_words = feature_names[top_toxic_ids]\n",
        "top_weights = weights[top_toxic_ids]\n",
        "\n",
        "# Выводим 10 самых токсичных слов\n",
        "print(\"\\nТоп-10 слов, считающихся токсичными моделью:\")\n",
        "for word, weight in zip(top_toxic_words, top_weights):\n",
        "    print(f\"{word}: {weight:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWHx4IKKPgs1",
        "outputId": "a248e93f-3358-49d4-ba52-110a7ddc8c5a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apples are stupid: 0.9994811959875193\n",
            "I love apples: 0.09864710468625032\n",
            "\n",
            "Топ-10 слов, считающихся токсичными моделью:\n",
            "stupid: 9.5496\n",
            "idiot: 8.6762\n",
            "idiots: 8.6292\n",
            "stupidity: 7.5847\n",
            "idiotic: 6.8082\n",
            "crap: 6.5476\n",
            "dumb: 6.5281\n",
            "pathetic: 6.4570\n",
            "morons: 6.3474\n",
            "moron: 6.3433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 7. Интерпретация результата (комментарий)\n",
        "--\n",
        "\n"
      ],
      "metadata": {
        "id": "cvCmovzOQwih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Слова в топе токсичности действительно являются оскорбительными.\n",
        "\n",
        "---\n",
        "Но, есть и нейтральные слова — это может указывать на смещение в данных или эффект контекста, возможно если мы доработаем связь контекста, слова как \"crap\" не будет в ТОП- 10.\n",
        "\n",
        "---\n",
        "Я проверил слова которые в US считаются ЭКСТРИМАЛЬНО токсичными, тут они выдают результаты не выше 0.4 - Что мне кажется подозрительно\n"
      ],
      "metadata": {
        "id": "8JfCQTGBRBpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 8. Проверка bias на чувствительные фразы\n",
        "--\n",
        "\n",
        "проверка, есть ли у модели предвзятость (bias) по религии или расе — важный шаг в этике AI."
      ],
      "metadata": {
        "id": "LMIDFa9sRDeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_phrases = [\n",
        "    \"\\nI have a muslim friend\",\n",
        "    \"I have a christian friend\",\n",
        "    \"I have an indian friend\",\n",
        "    \"I have a jew friend\",\n",
        "    \"\\nI have a black friend\",\n",
        "    \"I am proud to be black\",\n",
        "    \"\\nI have a homosexual friend\",\n",
        "    \"I have a lesbian friend\",\n",
        "    \"I have a bisexual friend\",\n",
        "    \"I have a transgender friend\",\n",
        "    \"I have a agender friend\"\n",
        "]\n",
        "\n",
        "print(\"\\nBias-тест на чувствительные фразы:\")\n",
        "for phrase in test_phrases:\n",
        "    score = predict_toxicity(phrase)\n",
        "    print(f\"{phrase:35s} → токсичность: {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhZv5u8zRaau",
        "outputId": "655a48c8-db8f-47a3-88f4-1a118575b4ff"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bias-тест на чувствительные фразы:\n",
            "\n",
            "I have a muslim friend             → токсичность: 0.445\n",
            "I have a christian friend           → токсичность: 0.122\n",
            "I have an indian friend             → токсичность: 0.196\n",
            "I have a jew friend                 → токсичность: 0.258\n",
            "\n",
            "I have a black friend              → токсичность: 0.515\n",
            "I am proud to be black              → токсичность: 0.495\n",
            "\n",
            "I have a homosexual friend         → токсичность: 0.453\n",
            "I have a lesbian friend             → токсичность: 0.347\n",
            "I have a bisexual friend            → токсичность: 0.215\n",
            "I have a transgender friend         → токсичность: 0.416\n",
            "I have a agender friend             → токсичность: 0.123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 9. Теоретический вывод\n",
        "--\n",
        "\n"
      ],
      "metadata": {
        "id": "ctvFKLXISa0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если модель систематически определяет, что комментарии с упоминанием \"muslim\", \"black\", \"homosexual, \"transgender\" и других идентичностей более токсичны, это указывает на наличие предвзятости (bias), которая заложена ещё на этапе разметки или сбора данных.\n",
        "\n",
        "Такой тип предвзятости можно отнести в первую очередь к Групповой по неосведомлённости (Fairness through unawareness), потому что даже если в обучении модели явно не использовалась информация о религии, расе или ориентации, прокси-признаки (например, сами слова \"muslim\", \"homosexual\", \"black\") всё ещё остаются в тексте.\n",
        "\n",
        "Таким образом, модель неявно учится ассоциировать упоминание определённых групп с токсичностью, даже если сами комментарии нейтральные или положительные.\n",
        "Кроме того, здесь можно наблюдать проявление исторической предвзятости (historical bias), когда данные отражают и закрепляют существующие в обществе стереотипы и несправедливости, а не исправляют их. Даже если мы строим идеальный алгоритм на \"грязных\" исторических данных, он будет повторять ошибки прошлого.\n",
        "\n",
        "Важно понимать, что модель в таком случае не просто ошибается — она усиливает уже существующие социальные дисбалансы о которых мы говорили на уроках. А это уже серьёзный этический риск при применении в реальных системах.\n",
        "\n",
        "---\n",
        "Таким образом, при проектировании моделей машинного обучения важно стремиться не только к высокой точности предсказаний,  \n",
        "но и уделять особое внимание выявлению и исправлению потенциальных источников несправедливости и предвзятости.  \n",
        "Только так можно создать действительно этичные и надёжные алгоритмы, которые будут служить всем пользователям без дискриминации."
      ],
      "metadata": {
        "id": "zHUnwjySSekn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 10. Как улучшить этичность модели\n",
        "---"
      ],
      "metadata": {
        "id": "0UdlH_n2gkcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "- Применить более сложные модели, обученные с регуляризацией fairness-aware (например, с ограничениями на дисбаланс TPR).\n",
        "---\n",
        "- Ввести двойную модель: одна предсказывает токсичность, другая — \"уверенность в нетоксичности\".\n",
        "Итоговое решение принимается на основе объединения предсказаний двух моделей:\n",
        "\n",
        "если обе уверены, что токсично — ок, токсично.\n",
        "\n",
        "если разногласие — отметить комментарий как \"неуверенный\" и отправить на доанализ.\n",
        "\n",
        "---\n",
        "\n",
        "- Вместо бинарного \"токсичен/нет\", сначала классифицировать комментарий по типу токсичности:\n",
        "\n",
        "              insult (оскорбление),\n",
        "\n",
        "              threat (угроза),\n",
        "\n",
        "              identity_attack (нападение на группу),\n",
        "\n",
        "              и только потом — финальная токсичность.\n",
        "\n",
        "Строим несколько моделей: одна определяет \"угроза или нет\", другая — \"атака на идентичность\", и т.д.\n",
        "\n",
        "Финальное решение комбинируем.\n",
        "\n",
        "---\n",
        " - Сила токсичности как дополнительный сигнал\n",
        "\n",
        "\n",
        "Использовать колонку severe_toxicity как вес к целевой переменной.\n",
        "\n",
        "Например: если обычная токсичность target > 0.7, но severe_toxicity тоже высоко - усиливаем уверенность в предсказании.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KEiKOA4Ugq5P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F79sSIRxgkHl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}